"""
[어텐션 Attention]
- 딥러닝은 생명의 신경, 뇌, 정신을 모방하여 인공지능을 완성시키는 방법에서 시작된 기술입니다.
    앞서 배운 내용으로 우리는 인간의 예측과 판단 능력(다층 퍼셉트론 구조), 그리고 학습 능력(오차 역전파)이 어떻게 구현 가능한지를 확인할 수 있었습니다.
    이것만 하더라도 기존 알고리즘에는 없는 유연한 예측과 학습 능력을 사용할 수 있게 되었습니다.
    여기에 더하여,
    생명체의 정신 활동을 이루는 또다른 요소들은 무엇이 있을까요?
    기억, 의식, 본능, 자아... 그리고 주목(Attention).
    여기서는 딥러닝 알고리즘에 큰 성능 향상을 이루게 해준 어텐선에 대해 알아 보겠습니다.

- 생명 활동의 기본 단위인 단백질이 어떤 구조와 방식으로 움직이는지 이해하는 것은 생명 현상을 연구하는 데 필수적입니다.
    특히 신약 개발 및 난치병 치료를 위해서는 단백질 구조를 알아야 하는데,
    이 구조를 알아내는 것은 간단하지 않습니다.
    그래서 이 분야의 연구를 하는 사람들이 2년에 한번씩 모여서 서로의 방법을 공유하고 평가하기로 하였는데,
    이것을 세계 단백질 구조 예측 대회라고 합니다.

    그런데 2020 년 12월, 제 14회 세계 단백질 구조 예측 대회에서, 대회에 참가한 모든 학자들이 깜짝 놀랄만한 일이 일어났습니다.
    누구도 흉내 낼 수 없을 만큼 정확하게 단백질 구조를 예측한 결과가 나왔고,
    그 예측을 해낸 것은 인간 학자가 아닌, 알파고를 만든 이력이 있는 구글의 딥마인드가 단백질 구조 예측을 위해 만든 인공지능인 알파폴드 2 였기 때문입니다.
    고도의 지적 확동이 가능한 학자들만이 가능하다고 여겨진 연구를 인공지능이 해내고,
    사람보다도 더 나은 결과를 냈다는 것은 기존에는 없는 성능 향상이 있었기 때문입니다.
    이러한 알파폴드 기능의 중요한 축을 담당하고 있는 것이 바로 어텐션(Attention)입니다.

- RNN 기본 설명
    기존에 자연어처리의 주역은 RNN 계열의 딥러닝 모델이었습니다.
    딥러닝의 유연한 예측 성능으로 시계열 데이터의 해석과 자연어 생성 기능을 구현할 수 있는 유일한 방법이었는데,
    기존의 다른 알고리즘보다 범용적이었고, 성능 역시 나쁘지 않았지만 만능이 아니었습니다.

    자연어 생성 부문에 언어 번역에 있어서도, 기존의 알고리즘에 비해서 완전히 대체 가능할 정도로 크게 기능이 향상된 것도 아니었습니다.
    제가 딥러닝 공부를 시작 한 것이 2020 년 후반인데,
    RNN 계열 모델로 만든 번역기의 성능이 그다지 좋지 않았었기에 자연어 처리 영역은 아직 시기상조라고 생각해서 CNN, YOLO 등의 컴퓨터 비전 분야를
    공부하기로 결정한 기억이 나네요. (당시 CNN 모델은 RNN 과 다르게 이미지 분석 분야에서 최고 성능을 이루었고, 활용 방법도 활발하게 연구되고 있었습니다.)

    어쨌든, 이러한 RNN 의 성능 저하의 원인은 RNN 모델의 원리를 알아야 설명이 가능합니다.
    앞선 RNN 정리글에서 이미 설명했지만 다시 한번 간단히 설명하겠습니다.

    RNN 모델은 기억을 구현해내려는 시도라고 할 수 있습니다.
    자연어 처리에서 입력값은 단어입니다.
    단어의 의미는 문맥에 따라 달라집니다.

    "차를 피해 인도로 갔다."
    "카레를 먹으러 인도로 갔다."

    위 두 문장에서 '인도'라는 것의 의미는 단어 그 자체만 봐서는 알 수 없습니다.
    심지어 두 문장의 '인도로 갔다' 라는 부분은 완전히 동일합니다.

    이때 필요한 것은, 앞 문맥이 되는 '차를 피해' 와 '카레를 먹으러' 라는 내용을,
    '인도'라는 단어를 해석할 때 까지 기억하는 것입니다.

    RNN 모델은 이러한 기억을 구현하기 위하여,
    문맥 벡터를 사용합니다.

    RNN 레이어에 처음 '차를' 이라는 단어가 들어갔을 때, 이것의 의미를 녹아낸 인코딩 벡터가 나올 것입니다.
    다음으로 '피해' 라는 단어를 입력할 때는, 앞선 단어인 '차를' 이라는 기억을 적용하기 위하여,
    앞서 출력한 벡터와 이번에 입력할 단어를 같이 넣어주어 인코딩을 하는 것입니다.

    이렇게 하면, 이전의 기억이자 단어의 의미를 뜻하는 '차를' 이라는 단어를 해석한 벡터를 기반으로, 여기에 '피해'라는 단어를 해석한 벡터가 나옵니다.
    이제 모델의 기억에는 '차를 피해' 라는 벡터가 저장된 상태죠?

    정확히는 '차를' 이라는 오래된 벡터와 '피해' 라는 새로운 단어 의미를 합친 데이터가 나온 것입니다.

    다음으로 '인도로' 라는 단어를 입력했을 때에는,
    '차를 피해' 라는 의미를 지닌 벡터를 기반으로 하여 '인도로' 라는 단어를 해석하게 될 것입니다.

    이로서, 이 문장에서 말하는 인도는 사람이 다니는 도로를 의미하는 인도라는 것을 알 수 있겠네요.

- RNN 의 한계 1 (초기 데이터 의미 소실)
    RNN 은 앞서 설명하였듯, 기억 공간을 하나만 가지고 있습니다.
    단편소설을 분석한다고 했을 때, 단어가 기억 공간에 중첩되고 계속 중첩될 것입니다.

    이는 마치 상자에 모래를 쌓아가는 것과 같습니다.
    상자에 모래를 넣고, 흔들어서 섞고,
    새로운 모래가 들어갈 만큼 기존 모래를 버리고, 새로운 모래를 넣고, 흔들어 섞고....
    이런식으로 하여, 최근의 정보일수록 이번의 판단에 큰 영향을 끼칠 것이지만, 오래된 정보일수록 그 영향력이 낮아질 것입니다.
    순서만을 고려하기 때문에 문맥상 중요한 단어건, 문맥상 의미없는 단어건 동일하게 섞이고, 흐릿해 집니다.

    예를들어,

    "자유를 되찾은 그날의 기쁨...
    아아,
    아아아아....!
    결코, 잊지 못하리..."

    위와 같은 문장에서 큰 의미를 지닌 '자유' 나 '기쁨' 같은 단어의 의미가,
    단순히 반복이 많이 된 '아'라는 단어에 의해 묻히게 되는 것입니다.

    이는 단순히 반복된 글자를 제거하는 간단한 데이터 전처리로 해결이 가능하다고 할 수도 있겠지만,
    위와 같이 노골적인 예시가 아니라,
    문장은 그럴싸 해보이지만, 문맥상 필요없는 내용이 들어갔다고 하면, 그것을 걸러내는 것도 어려울 것입니다.

    이것이 RNN 의 근본적인 문제입니다.

    물론, 이것을 해결하기 위한 방법이 금방 제시되었습니다.
    장기 기억(Long Term Memory) 과 단기 기억(Short Term Memory) 으로 기억 영역을 2개로 늘리고,
    이번 데이터를 분석하여 다음에 넘겨줄 기억에 얼마나 보관할지에 대한 영향력을 스스로 계산하는 LSTM 이라는 방식입니다.

    이로써 RNN 의 성능은 확연히 나아졌습니다.
    다만, 이것으로 모든 것이 해결 된 것은 아니며,
    여전히 혁신이라 불릴 만큼의 성능까지는 미치지 못했습니다.

    LSTM 설명글에서 설명한 구조를 보면 알 수 있듯이,
    이 역시 모래상자 위로 모래를 쌓는 구조에서 시작하여, 단지 모래를 쌓을 때, 이번 데이터의 모래를 얼마나 쌓을지에 대한 계산 기능을 넣은 것일 뿐,
    모래를 쌓음으로써 생겨나는 초기 데이터의 의미 약화라는 근본적인 문제를 해결하지는 못했다는 것입니다.

- RNN 의 한계 2 (기억의 방향)
    RNN 의 문제는 초기 데이터의 의미 약화 문제만 있는 것이 아닙니다.

    예를 들어 설명 하겠습니다.

    "인도로 갔다. 차를 피해서..."

    위와 같은 문장을 앞선 알고리즘에 넣는다면,
    앞선 RNN 알고리즘은 앞의 단어로만 뒤의 단어를 해석하기에 인도의 의미를 알 수 없을 것입니다.

    이것을 해결하려면 어떻게 할까요?

    RNN 을 역순으로 실행하면 됩니다.
    RNN 을 일단 정방향으로 실행시켜 기억 벡터들을 만들고,
    RNN 을 역순으로 실행시켜 또 그것의 기억 벡터들을 만들고,
    기억 벡터들을 조합하여 사용하면 됩니다.

    이것을 양방향 RNN 이라고 합니다.

    직관적으로 알 수 있겠지만, 여기에도 문제는 있네요.
    순방향과 역방향을 모두 실행해야 하기 때문에, RNN 의 장점이라고 할 수 있는, 무한한 시계열 데이터에 대한 실시간 분석이 불가능해지고,
    역방향을 위하여 어떻게든 끝을 정해두어야 한다는 것과, 연산량이 일반 RNN 의 두배가 된다는 것입니다.

    이로써 RNN 의 한계를 알아보았습니다.
    이외에는 RNN 의 경우 입력값의 길이에 따라서 레이어가 길어지는 효과가 있으므로 학습에 기울기 소실이 일어날 수 있다는 문제가 있습니다.

    모든 문제마다 나름의 해결법이 발명되고, 각 증상을 완화시켰는데,
    조금씩 부족한 것은 사실입니다.

    이러한 상황에서, 기존 RNN 이 놓치고 있던 개념을 찌른 방법이 나타났는데,
    이것이 바로 어텐션입니다.

-


"""

import torch
import torch.nn as nn
import torch.optim as optim

class LSTMWithAttention(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=1):
        super(LSTMWithAttention, self).__init__()

        # LSTM 레이어를 정의합니다.
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

        # 어텐션 레이어를 정의합니다.
        self.attention = nn.Linear(hidden_size, 1)

        # 출력 레이어를 정의합니다.
        self.output = nn.Linear(hidden_size, 1)  # 예제에서는 출력 크기가 1

    def forward(self, x):
        # LSTM을 실행합니다.
        lstm_out, _ = self.lstm(x)

        # 어텐션 점수를 계산합니다.
        attention_scores = self.attention(lstm_out)
        attention_weights = torch.softmax(attention_scores, dim=1)  # 소프트맥스로 정규화합니다.

        # 어텐션 가중치로 LSTM의 출력에 가중치를 적용합니다.
        context_vector = torch.sum(lstm_out * attention_weights, dim=1)

        # 출력 레이어를 통과시킵니다.
        output = self.output(context_vector)

        return output


"""
사실 분류 모델에서의 파라미터가 어텐션이라 할 수 있습니다.
N 개의 데이터가 존재하면, 그것이 특정 결정 노드를 활성화 하는데 얼마만큼의 영향력을 가지는지에 대한 '가중치' 가 파라미터이기 때문이죠.
위의 어텐션은 입력값이 직렬로 들어오는 시계열 데이터를 병렬적으로 판단하도록 한 것 뿐입니다.
"""
