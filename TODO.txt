- 15번 글부터 재정리

- latent_vector 나 weight 를 파일로 저장해두고 활용하여 모델 성능이나 확장성을 늘릴 수 있을까?
    사람이 뚜렷하지 않은 기억을 깊이 감춰두거나 회상하는 것 처럼...
    혹은 책이나 자료를 보고 흐릿한 지식을 보충하는 것처럼

- 언어 전처리 시에 의미를 기반으로 한 보다 근본적인 전처리로 문자 부호에 독립된 처리를 할 수 있을까?
    단어 의미를 latent vector 로 자동으로 학습하도록 신경망 설계를 해볼것

- 사람이 언어를 받아들일 때에는 글자 그대로 받아들이는게 아니라 읽어봄(소리로 받아들임)
    그리고 자주 접한 글자와 아닌 글자가 있음.
    사람은 처음 본 글자는 읽어보려고(tts 생성) 함.
    이것은 즉 글자에 대응하는 음성 데이터를 어디에 저장하고 있다는 것.

- 어떤 단어를 배운다는 것은 그것이 ? 에서 시작해서 점차 다른 관점의 다른 평가를 쌓아가는 것.

- 하위 단어 토크나이징이 아니라 글자단위로 받아서 어텐션을 적용할수 있을지...
    글자 시퀀스를 시계열로 받아서 이것으로 클러스터링을 하고 그 결과를 다시 입력값으로 입력하여 자연어 처리

- 튜토리얼 작성이 끝나면 쓸만한 기술들을 추려서 개발 템플릿 만들기

- 모델 구역을 나누어서 머신 스스로 모델을 만들도록 유도해보기

- 완전 학습이 아니라, 기억 부분을 추가하여 실시간 학습하도록 해보기
    파라미터 학습이 아니라,
    애초에 입력값에 '기억'을 담당하는 벡터 부분을 같이 넣어주고, 이 기억 벡터를 갱신하면 결과값이 변경되도록 유도
    기억 벡터는 사용자가 넣어줄수도 있지만, 보통 머신이 스스로 인코딩 해서 입력하도록...
    기억 벡터는 여러개로 늘어날 수도 있음(한계 설정을 정하기). 이것을 모델이 검색하도록 학습하기

- 미완성 :
    4(통계 관련) - 빅데이터 아카데미 고급반 내용 정리,
    5(머신러닝 관련) - 딥러닝 이외의 머신러닝 정리,
    14(퍼셉트론) - 역전파 관련 설명과 예시 추가, 옵티마이저 정리,
    15(데이터 처리) - 판다스, 맷플롯립, 시각화
    28(워드피스),
    31(언어모델) - https://wikidocs.net/217248 에서부터 참고하여 채워넣기,
    35(네거티브 샘플링),
    37(글로브(Global Vectors for Word Representation, GloVe)),
    43(ELMo)

- VAE, AAE, GAN 시리즈, Diffusion 시리즈 등...